{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Stolcke PCFG Parser","text":"<p>A compact implementation of the probabilistic Earley parser (Stolcke 1994/1995) with prefix probabilities, written in Python. It provides:</p> <ul> <li>A normalized PCFG model with log-space probabilities.</li> <li>An Earley-style parser that tracks forward (alpha) and inner (gamma) scores.</li> <li>Prefix probability for incremental decoding, plus an adapter for constrained LLM generation.</li> <li>A small demo CLI and a simple smoke test.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Local dev (preferred): <code>make setup &amp;&amp; make sync</code></li> <li>Direct with uv: <code>uv venv &amp;&amp; uv sync</code></li> </ul> <p>Requires Python 3.12+.</p>"},{"location":"#quickstart-library","title":"Quickstart (Library)","text":"<pre><code>from stolcke_pcfg import PCFG, StolckeParser\n\nG = PCFG([\n    (\"S\", [\"S\", \"a\"], 0.4),\n    (\"S\", [\"a\"], 0.6),\n])\nP = StolckeParser(G, \"S\")\n\nprint(P.allowed_terminals())   # {\"a\"}\nP.step(\"a\")                    # True\nprint(P.prefix_logprob())      # log P(prefix)\nprint(P.accepted())            # True after \u22651 token\n</code></pre>"},{"location":"#quickstart-cli","title":"Quickstart (CLI)","text":"<ul> <li>Run default demo: <code>stolcke-parser</code></li> <li>Provide tokens: <code>stolcke-parser a a a</code></li> </ul> <p>See docs/cli.md for details.</p>"},{"location":"#key-concepts","title":"Key Concepts","text":"<ul> <li>PCFG: Rules <code>lhs -&gt; rhs</code> with probabilities normalized per <code>lhs</code>.</li> <li>Earley items: dotted rules plus start index; scan/predict/complete.</li> <li>Prefix probability: sum of alpha over scanned states at each prefix.</li> </ul> <p>For design and API details, see docs/architecture.md and docs/api.md.</p>"},{"location":"adapter/","title":"Constrained Decoding Adapter","text":"<p>The <code>ConstrainedDecoderAdapter</code> bridges the parser and a tokenizer-based decoder by translating allowed terminals into token IDs.</p>"},{"location":"adapter/#usage","title":"Usage","text":"<pre><code>from stolcke_pcfg import ConstrainedDecoderAdapter, PCFG, StolckeParser\n\n# Build grammar and parser\nG = PCFG([...])\nP = StolckeParser(G, \"S\")\n\n# Tokenizer bridges\nid2s = lambda i: your_tokenizer.decode([i])\ns2id = lambda s: your_tokenizer.encode(s)[0] if s in your_vocab else None\n\nadapter = ConstrainedDecoderAdapter(P, id2s, s2id)\nmask = adapter.allowed_token_ids()  # set[int]\n</code></pre> <ul> <li>Call <code>adapter.allowed_token_ids()</code> at each decoding step to mask logits.</li> <li>After sampling/choosing a token ID, call <code>adapter.step_with_token(token_id)</code> to advance the parser.</li> <li>You may provide <code>next_token_filter(terms: set[str]) -&gt; set[int]</code> to customize which tokens are allowed (e.g., subword boundary handling).</li> </ul>"},{"location":"adapter/#notes","title":"Notes","text":"<ul> <li>Terminals must correspond to full tokens in your vocabulary; otherwise, insert a filter/mapper that enforces only vocabulary-aligned terminals or composes multi-token terminals.</li> <li>Parser provides <code>allowed_terminals()</code> directly if you do not need IDs.</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>High-level imports are available from <code>stolcke_pcfg</code>.</p> <pre><code>from stolcke_pcfg import PCFG, Rule, StolckeParser, ConstrainedDecoderAdapter\n</code></pre>"},{"location":"api/#grammarrule","title":"grammar.Rule","text":"<ul> <li>Fields: <code>lhs: str</code>, <code>rhs: tuple[str, ...]</code>, <code>logp: float</code> (log probability).</li> <li>String form: <code>\"A -&gt; a b (0.123456)\"</code> with linear-space probability for readability.</li> </ul>"},{"location":"api/#grammarpcfg","title":"grammar.PCFG","text":"<ul> <li><code>PCFG(rules: Iterable[tuple[str, Iterable[str], float]])</code></li> <li>Builds and normalizes rules per LHS; <code>float</code> probabilities must be <code>&gt; 0</code>.</li> <li><code>rules_for(lhs: str) -&gt; list[Rule]</code></li> <li><code>nonterminals -&gt; frozenset[str]</code></li> <li><code>is_terminal(sym: str) -&gt; bool</code></li> </ul>"},{"location":"api/#stolcke_parserstolckeparser","title":"stolcke_parser.StolckeParser","text":"<ul> <li><code>StolckeParser(grammar: PCFG, start_symbol: str)</code></li> <li>Validates unsupported productions; creates augmented start <code>S' -&gt; S</code>.</li> <li><code>reset() -&gt; None</code></li> <li><code>allowed_terminals() -&gt; set[str]</code></li> <li><code>step(terminal: str) -&gt; bool</code></li> <li>Advances one token if allowed; updates prefix probability.</li> <li><code>prefix_logprob() -&gt; float</code></li> <li><code>accepted() -&gt; bool</code></li> </ul>"},{"location":"api/#constrained_adapterconstraineddecoderadapter","title":"constrained_adapter.ConstrainedDecoderAdapter","text":"<ul> <li><code>ConstrainedDecoderAdapter(parser, token_id_to_str, str_to_token_id, next_token_filter=None)</code></li> <li><code>allowed_token_ids() -&gt; set[int]</code>: current mask of allowed next token IDs.</li> <li><code>step_with_token(token_id: int) -&gt; bool</code>: advance via token ID.</li> <li>Optional <code>next_token_filter(terms: set[str]) -&gt; set[int]</code> can pre-filter allowed IDs.</li> </ul>"},{"location":"api/#notes","title":"Notes","text":"<ul> <li>All probabilities are accumulated in log-space. Use <code>math.exp(logp)</code> to inspect linear values.</li> <li>The internal <code>alpha</code>/<code>gamma</code> arrays live in <code>probabilities.ProbChart</code> and are not part of the public API surface.</li> </ul>"},{"location":"architecture/","title":"Architecture","text":"<p>This package implements a probabilistic Earley parser with Stolcke-style forward (alpha) and inner (gamma) probabilities. Components are small and focused.</p>"},{"location":"architecture/#modules","title":"Modules","text":"<ul> <li><code>grammar.py</code></li> <li><code>Rule</code>: immutable production with <code>lhs</code>, <code>rhs: tuple[str, ...]</code>, and <code>logp</code>.</li> <li><code>PCFG</code>: normalizes rule probabilities per LHS; identifies terminals vs nonterminals.</li> <li><code>earley_core.py</code></li> <li><code>EarleyItem</code>: <code>(rule, dot, start)</code>; supports <code>next_symbol()</code>, <code>advance()</code>, <code>finished()</code>.</li> <li><code>BP</code>: minimal backpointer info for provenance.</li> <li><code>EarleyChart</code>: per-position maps of items \u2192 best Viterbi score and backpointers.</li> <li><code>probabilities.py</code></li> <li><code>ProbChart</code>: extends chart with <code>alpha</code> (forward) and <code>gamma</code> (inner) maps; log-sum accumulation.</li> <li><code>stolcke_parser.py</code></li> <li><code>StolckeParser</code>: orchestrates SCAN, PREDICT, COMPLETE, keeps prefix probability, exposes <code>allowed_terminals()</code>.</li> <li><code>constrained_adapter.py</code></li> <li><code>ConstrainedDecoderAdapter</code>: maps allowed terminals to token IDs for LLM decoding.</li> </ul>"},{"location":"architecture/#algorithm-highlights","title":"Algorithm Highlights","text":"<ul> <li>Items: dotted <code>Rule</code> with origin <code>start</code> index. State sets exist at each input position <code>k</code>.</li> <li>Predictor: when item expects a nonterminal <code>Y</code>, add <code>Y -&gt; \u2022 \u03b3</code> at position <code>k</code>; update alpha and initialize gamma with rule prob.</li> <li>Scanner: when item expects terminal <code>a</code>, consuming <code>a</code> at <code>k</code> yields <code>\u2022</code> advanced item at <code>k+1</code>; propagate alpha/gamma.</li> <li>Completer: when an item finishes at <code>k</code>, combine with parents waiting for its LHS at their origin <code>i</code>; propagate alpha/gamma.</li> <li>Prefix probability: at each scan, sum alphas of scanned states at the new position.</li> </ul>"},{"location":"architecture/#constraints-and-assumptions","title":"Constraints and Assumptions","text":"<ul> <li>No epsilon or unit productions (planned extension); left recursion allowed.</li> <li>Probabilities are in log-space; <code>LOG_ZERO</code> sentinel avoids <code>-inf</code> arithmetic.</li> <li>Terminals are symbols that never appear on any LHS.</li> </ul>"},{"location":"architecture/#complexity","title":"Complexity","text":"<ul> <li>Worst-case cubic in input length for general CFGs (Earley), with grammar-dependent constants. Alpha/gamma bookkeeping adds overhead but remains linear in number of state updates.</li> </ul>"},{"location":"cli/","title":"CLI","text":"<p>The package exposes a small demo CLI for experimentation.</p>"},{"location":"cli/#command","title":"Command","text":"<ul> <li>Entry point: <code>stolcke-parser</code></li> <li>Arguments: a sequence of space-separated tokens (optional). If omitted, defaults to <code>a a a</code>.</li> </ul>"},{"location":"cli/#examples","title":"Examples","text":"<pre><code>stolcke-parser\nstolcke-parser a a a\nstolcke-parser a a b   # will stop when an unexpected token is encountered\n</code></pre> <p>The CLI reports: - Allowed terminals at the start. - For each token: whether it advanced, and the current prefix log-probability. - Final acceptance status of the consumed prefix.</p> <p>Note: The CLI uses a built-in left-recursive grammar <code>S -&gt; S 'a' | 'a'</code>. For custom grammars and programmatic usage, use the library API.</p>"},{"location":"development/","title":"Development Guide","text":""},{"location":"development/#environment","title":"Environment","text":"<ul> <li>Create venv and install deps: <code>make setup</code> (or <code>uv venv &amp;&amp; uv sync</code>).</li> <li>Install hooks: <code>make hooks</code>.</li> </ul>"},{"location":"development/#common-tasks","title":"Common Tasks","text":"<ul> <li>Lint: <code>make lint</code></li> <li>Format: <code>make fmt</code></li> <li>Test: <code>make test</code></li> <li>Coverage: <code>make coverage</code></li> <li>Run CLI: <code>make run</code> (demo grammar)</li> </ul> <p>Direct equivalents: <code>uv run ruff check .</code>, <code>uv run ruff format .</code>, <code>uv run pytest</code>.</p>"},{"location":"development/#conventions","title":"Conventions","text":"<ul> <li>Style: Managed by Ruff (checking + formatting). Line length 100.</li> <li>Naming: modules <code>lower_snake_case</code>, classes <code>PascalCase</code>, functions/vars <code>lower_snake_case</code>.</li> <li>Imports: grouped/sorted; <code>stolcke_pcfg</code> is first-party.</li> <li>Commits: Conventional Commits (e.g., <code>feat(parser): add tokenizer</code>).</li> </ul>"},{"location":"development/#testing","title":"Testing","text":"<ul> <li>Framework: <code>pytest</code> with mirrors of <code>src/</code> structure in <code>tests/</code>.</li> <li>Add deterministic tests; prefer small, isolated fixtures under <code>tests/fixtures/</code>.</li> <li>For probability checks, compare in log-space; avoid fragile float equality.</li> </ul>"},{"location":"development/#notes","title":"Notes","text":"<ul> <li>The parser currently rejects epsilon and unit productions; plan tests accordingly.</li> <li>In restricted environments, running <code>make lint</code> via <code>uv</code> may fail; use the venv tools directly (e.g., <code>.venv/bin/ruff</code>).</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>This repository ships with a simple prefix-probability demo.</p>"},{"location":"examples/#demo_prefix_probpy","title":"demo_prefix_prob.py","text":"<p>Path: <code>examples/demo_prefix_prob.py</code></p> <p>Demonstrates a left-recursive grammar <code>S -&gt; S 'a' | 'a'</code> and prints: - Allowed terminals at the start. - For each consumed token: whether the parser advanced and the current prefix log-probability. - Whether the current prefix is accepted.</p> <p>Run it with:</p> <pre><code>uv run python examples/demo_prefix_prob.py\n</code></pre> <p>or try the CLI:</p> <pre><code>uv run stolcke-parser a a a\n</code></pre> <p>You can adapt the example to different grammars by constructing <code>PCFG([...])</code> and <code>StolckeParser(G, \"S\")</code> manually.</p>"},{"location":"grammar/","title":"Grammar Model","text":"<p>The <code>PCFG</code> class represents a probabilistic context-free grammar with per-LHS normalization.</p>"},{"location":"grammar/#rules","title":"Rules","text":"<p>Create a grammar from triplets <code>(lhs, rhs, p)</code> where: - <code>lhs: str</code> - <code>rhs: Iterable[str]</code> (use empty for epsilon only when supported; current parser forbids epsilon/unit) - <code>p: float</code>, <code>p &gt; 0</code></p> <p>Example:</p> <pre><code>G = PCFG([\n    (\"S\", [\"NP\", \"VP\"], 0.9),\n    (\"S\", [\"VP\"], 0.1),\n    (\"NP\", [\"Det\", \"N\"], 1.0),\n    (\"VP\", [\"V\", \"NP\"], 0.5),\n    (\"VP\", [\"V\"], 0.5),\n    (\"Det\", [\"the\"], 0.6),\n    (\"Det\", [\"a\"], 0.4),\n    (\"N\", [\"cat\"], 0.7),\n    (\"N\", [\"mat\"], 0.3),\n    (\"V\", [\"sleeps\"], 0.5),\n    (\"V\", [\"likes\"], 0.5),\n])\n</code></pre> <p>Probabilities are normalized per LHS during construction.</p>"},{"location":"grammar/#terminals-vs-nonterminals","title":"Terminals vs Nonterminals","text":"<ul> <li>Nonterminals are exactly the symbols that appear as an LHS.</li> <li>Terminals are symbols that never appear as an LHS.</li> <li>Current parser limitations: no epsilon productions and no unit productions (e.g., <code>A -&gt; B</code>). Left recursion is supported.</li> </ul>"}]}