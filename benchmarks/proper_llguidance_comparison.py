#!/usr/bin/env python3
"""
Proper llguidance comparison using their grammar-based approach.

Based on https://guidance-ai.github.io/llguidance/llg-go-brrr
"""

import time
import torch
import json
from dataclasses import dataclass
from typing import Optional, Dict, Any
from transformers import AutoTokenizer, AutoModelForCausalLM

# Stolcke imports
from stolcke_pcfg import PCFG, StolckeParser
from stolcke_pcfg.robust_adapter import RobustConstrainedAdapter

# llguidance imports
import llguidance


@dataclass
class BenchmarkResult:
    """Result from a generation benchmark."""
    library: str
    success: bool
    generation_time: float
    tokens_generated: int
    output: str
    valid_json: bool
    has_probabilities: bool
    log_probability: Optional[float] = None
    error: Optional[str] = None


class ProperLLGuidanceComparison:
    """Proper comparison using llguidance's grammar-based approach."""
    
    def __init__(self, device=None):
        if device is None:
            if torch.backends.mps.is_available():
                device = torch.device("mps")
            else:
                device = torch.device("cpu")
        self.device = device
    
    def create_llguidance_grammar(self) -> str:
        """Create llguidance grammar following their documentation."""
        return """
start: person_json

person_json: %json {
    "type": "object",
    "properties": {
        "name": { "type": "string", "enum": ["Alice", "Bob", "Charlie"] },
        "email": { "type": "string", "enum": ["alice@domain.com", "bob@company.org"] },
        "age": { "type": "number", "enum": [25, 30, 35] }
    },
    "additionalProperties": false,
    "oneOf": [
        {"required": ["name"]},
        {"required": ["email"]},
        {"required": ["age"]}
    ]
}
"""
    
    def create_stolcke_grammar(self) -> PCFG:
        """Create equivalent Stolcke PCFG grammar."""
        return PCFG([
            ("S", ["{", "Property", "}"], 1.0),
            ("Property", ["NameProp"], 0.33),
            ("Property", ["EmailProp"], 0.33),
            ("Property", ["AgeProp"], 0.34),
            ("NameProp", ['"name"', ":", "NameValue"], 1.0),
            ("EmailProp", ['"email"', ":", "EmailValue"], 1.0),
            ("AgeProp", ['"age"', ":", "AgeValue"], 1.0),
            ("NameValue", ['"Alice"'], 0.33),
            ("NameValue", ['"Bob"'], 0.33),
            ("NameValue", ['"Charlie"'], 0.34),
            ("EmailValue", ['"alice@domain.com"'], 0.5),
            ("EmailValue", ['"bob@company.org"'], 0.5),
            ("AgeValue", ["25"], 0.33),
            ("AgeValue", ["30"], 0.33),
            ("AgeValue", ["35"], 0.34),
        ])
    
    def test_stolcke_generation(self, model, tokenizer, max_tokens: int = 20) -> BenchmarkResult:
        """Test Stolcke PCFG generation."""
        
        start_time = time.perf_counter()
        
        try:
            grammar = self.create_stolcke_grammar()
            parser = StolckeParser(grammar, "S")
            
            def id2str(tid): return tokenizer.decode([tid], clean_up_tokenization_spaces=False)
            def str2id(s):
                ids = tokenizer.encode(s, add_special_tokens=False)
                return ids[0] if len(ids) == 1 else None
            
            adapter = RobustConstrainedAdapter(
                parser=parser,
                token_id_to_str=id2str,
                str_to_token_id=str2id,
                tokenizer=tokenizer
            )
            
            generated = torch.empty((1, 0), dtype=torch.long, device=self.device)
            
            for step in range(max_tokens):
                # Model input
                if generated.size(1) == 0:
                    input_ids = torch.tensor([[tokenizer.bos_token_id]], device=self.device)
                else:
                    input_ids = generated
                
                # Forward pass
                with torch.no_grad():
                    outputs = model(input_ids=input_ids)
                    logits = outputs.logits[:, -1, :]
                
                # Apply constraints
                allowed = adapter.allowed_token_ids()
                if not allowed:
                    break
                
                mask = adapter.allowed_token_mask(vocab_size=logits.size(-1))
                logits[0, ~torch.tensor(mask, dtype=torch.bool, device=self.device)] = -1e30
                
                # Sample
                probs = torch.softmax(logits[0] / 0.8, dim=-1)
                next_token = torch.multinomial(probs, 1).item()
                
                if not adapter.step_with_token(next_token):
                    break
                
                if generated.size(1) == 0:
                    generated = torch.tensor([[next_token]], device=self.device)
                else:
                    generated = torch.cat([generated, torch.tensor([[next_token]], device=self.device)], dim=-1)
                
                if parser.accepted():
                    break
            
            generation_time = time.perf_counter() - start_time
            output = tokenizer.decode(generated[0], skip_special_tokens=True)
            
            # Validate JSON
            valid_json = False
            try:
                json.loads(output)
                valid_json = True
            except:
                pass
            
            # Get log probability
            log_prob = None
            if parser.accepted():
                try:
                    log_prob = parser.sentence_logprob()
                except:
                    pass
            
            return BenchmarkResult(
                library="Stolcke PCFG",
                success=parser.accepted(),
                generation_time=generation_time,
                tokens_generated=generated.size(1),
                output=output,
                valid_json=valid_json,
                has_probabilities=True,
                log_probability=log_prob
            )
            
        except Exception as e:
            generation_time = time.perf_counter() - start_time
            return BenchmarkResult(
                library="Stolcke PCFG",
                success=False,
                generation_time=generation_time,
                tokens_generated=0,
                output="",
                valid_json=False,
                has_probabilities=True,
                error=str(e)
            )
    
    def test_llguidance_generation(self, model, tokenizer, max_tokens: int = 20) -> BenchmarkResult:
        """Test llguidance generation using their proper API."""
        
        start_time = time.perf_counter()
        
        try:
            # Create grammar using their approach
            grammar_str = self.create_llguidance_grammar()
            
            # Try to create the llguidance components
            # We need to figure out the proper API from the available classes
            
            # Option 1: Try LLInterpreter with different constructors
            try:
                # Create tokenizer - try different approaches
                try:
                    ll_tokenizer = llguidance.LLTokenizer(tokenizer)
                except:
                    ll_tokenizer = llguidance.TokenizerWrapper(tokenizer)
                
                # Compile grammar
                try:
                    # Try grammar_from function mentioned in the module
                    compiled_grammar = llguidance.grammar_from(grammar_str, format=llguidance.GrammarFormat.LARK)
                except:
                    # Try LarkCompiler
                    compiler = llguidance.LarkCompiler()
                    compiled_grammar = compiler.compile(grammar_str)
                
                # Create interpreter
                interpreter = llguidance.LLInterpreter(
                    ll_tokenizer=ll_tokenizer,
                    grammar=compiled_grammar
                )
                
                print("‚úÖ llguidance components created successfully")
                
            except Exception as setup_error:
                print(f"‚ùå llguidance setup failed: {setup_error}")
                raise setup_error
            
            # Generation loop
            generated = []
            
            for step in range(max_tokens):
                try:
                    # Get mask from llguidance
                    mask_result = interpreter.compute_mask()
                    
                    if hasattr(mask_result, 'sample_mask') and mask_result.sample_mask:
                        allowed_tokens = [i for i, allowed in enumerate(mask_result.sample_mask) if allowed]
                    else:
                        # Try alternative API
                        allowed_tokens = getattr(mask_result, 'allowed_tokens', [])
                    
                    if not allowed_tokens:
                        break
                    
                    # Model forward pass
                    if not generated:
                        input_ids = torch.tensor([[tokenizer.bos_token_id]], device=self.device)
                    else:
                        input_ids = torch.tensor([generated], device=self.device)
                    
                    with torch.no_grad():
                        outputs = model(input_ids=input_ids)
                        logits = outputs.logits[:, -1, :]
                    
                    # Apply mask
                    mask = torch.zeros(logits.size(-1), dtype=torch.bool, device=self.device)
                    for token_id in allowed_tokens:
                        if 0 <= token_id < logits.size(-1):
                            mask[token_id] = True
                    
                    if not mask.any():
                        break
                    
                    logits[0, ~mask] = -1e30
                    
                    # Sample
                    probs = torch.softmax(logits[0] / 0.8, dim=-1)
                    next_token = torch.multinomial(probs, 1).item()
                    
                    # Advance llguidance
                    advance_result = interpreter.advance_parser(next_token)
                    if hasattr(advance_result, 'stop') and advance_result.stop:
                        break
                    
                    generated.append(next_token)
                    
                    # Check for completion
                    if hasattr(advance_result, 'is_accepting') and advance_result.is_accepting():
                        break
                        
                except Exception as step_error:
                    print(f"   llguidance step {step} failed: {step_error}")
                    break
            
            generation_time = time.perf_counter() - start_time
            output = tokenizer.decode(generated, skip_special_tokens=True)
            
            # Validate JSON
            valid_json = False
            try:
                json.loads(output)
                valid_json = True
            except:
                valid_json = output.strip().startswith('{') and output.strip().endswith('}')
            
            return BenchmarkResult(
                library="llguidance",
                success=valid_json and len(generated) > 0,
                generation_time=generation_time,
                tokens_generated=len(generated),
                output=output,
                valid_json=valid_json,
                has_probabilities=False
            )
            
        except Exception as e:
            generation_time = time.perf_counter() - start_time
            return BenchmarkResult(
                library="llguidance",
                success=False,
                generation_time=generation_time,
                tokens_generated=0,
                output="",
                valid_json=False,
                has_probabilities=False,
                error=str(e)
            )
    
    def run_comparison(self, model_name: str = "gpt2", trials: int = 3):
        """Run the comprehensive comparison."""
        
        print("ü•ä Stolcke PCFG vs llguidance - PRODUCTION COMPARISON")
        print("üéØ Grammar-Constrained Generation Battle")
        print("=" * 60)
        
        # Load model
        print(f"Loading {model_name}...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(model_name)
        model = model.to(self.device)
        model.eval()
        print("‚úÖ Model loaded")
        
        results = {'stolcke': [], 'llguidance': []}
        
        for trial in range(trials):
            print(f"\nüîÑ Trial {trial + 1}/{trials}")
            print("-" * 25)
            
            # Test Stolcke
            print("Testing Stolcke PCFG...")
            stolcke_result = self.test_stolcke_generation(model, tokenizer)
            results['stolcke'].append(stolcke_result)
            
            status = "‚úÖ" if stolcke_result.success else "‚ùå"
            print(f"  {status} {stolcke_result.tokens_generated} tokens, "
                  f"{stolcke_result.generation_time*1000:.0f}ms")
            print(f"     Output: '{stolcke_result.output}'")
            if stolcke_result.error:
                print(f"     Error: {stolcke_result.error}")
            
            # Test llguidance
            print("Testing llguidance...")
            llguidance_result = self.test_llguidance_generation(model, tokenizer)
            results['llguidance'].append(llguidance_result)
            
            status = "‚úÖ" if llguidance_result.success else "‚ùå"
            print(f"  {status} {llguidance_result.tokens_generated} tokens, "
                  f"{llguidance_result.generation_time*1000:.0f}ms")
            print(f"     Output: '{llguidance_result.output}'")
            if llguidance_result.error:
                print(f"     Error: {llguidance_result.error}")
        
        self.analyze_results(results)
        return results
    
    def analyze_results(self, results: Dict[str, list]):
        """Analyze and present comparison results."""
        
        print(f"\nüìä FINAL COMPARISON ANALYSIS")
        print("=" * 40)
        
        stolcke_results = results['stolcke']
        llguidance_results = results['llguidance']
        
        # Success rates
        stolcke_success_rate = sum(r.success for r in stolcke_results) / len(stolcke_results)
        llguidance_success_rate = sum(r.success for r in llguidance_results) / len(llguidance_results)
        
        # Average metrics for successful runs
        def avg_metric(results_list, metric):
            successful = [r for r in results_list if r.success]
            if not successful:
                return 0
            values = [getattr(r, metric) for r in successful]
            return sum(values) / len(values)
        
        stolcke_avg_time = avg_metric(stolcke_results, 'generation_time') * 1000
        llguidance_avg_time = avg_metric(llguidance_results, 'generation_time') * 1000
        
        stolcke_avg_tokens = avg_metric(stolcke_results, 'tokens_generated')
        llguidance_avg_tokens = avg_metric(llguidance_results, 'tokens_generated')
        
        print("| Metric | Stolcke PCFG | llguidance | Winner |")
        print("|--------|--------------|------------|--------|")
        print(f"| Success Rate | {stolcke_success_rate:>10.0%} | {llguidance_success_rate:>8.0%} | {'Stolcke' if stolcke_success_rate > llguidance_success_rate else 'llguidance' if llguidance_success_rate > stolcke_success_rate else 'Tie':>6} |")
        
        if stolcke_avg_time > 0 and llguidance_avg_time > 0:
            speed_winner = "llguidance" if llguidance_avg_time < stolcke_avg_time else "Stolcke" if stolcke_avg_time < llguidance_avg_time else "Tie"
            print(f"| Avg Gen Time | {stolcke_avg_time:>7.0f}ms | {llguidance_avg_time:>6.0f}ms | {speed_winner:>6} |")
        elif stolcke_avg_time > 0:
            print(f"| Avg Gen Time | {stolcke_avg_time:>7.0f}ms | {'Failed':>6} | Stolcke |")
        elif llguidance_avg_time > 0:
            print(f"| Avg Gen Time | {'Failed':>7} | {llguidance_avg_time:>6.0f}ms | llguidance |")
        
        if stolcke_avg_tokens > 0 and llguidance_avg_tokens > 0:
            print(f"| Avg Tokens | {stolcke_avg_tokens:>10.1f} | {llguidance_avg_tokens:>8.1f} | {'Similar':>6} |")
        
        print(f"| Probabilities | {'Yes':>10} | {'No':>8} | Stolcke |")
        
        # Example outputs
        print(f"\nüìù EXAMPLE OUTPUTS")
        print("-" * 20)
        
        successful_stolcke = [r for r in stolcke_results if r.success and r.output.strip()]
        successful_llguidance = [r for r in llguidance_results if r.success and r.output.strip()]
        
        if successful_stolcke:
            result = successful_stolcke[0]
            print(f"Stolcke PCFG: '{result.output}' (JSON: {'‚úÖ' if result.valid_json else '‚ùå'})")
            if result.log_probability is not None:
                print(f"              Log-probability: {result.log_probability:.4f}")
        
        if successful_llguidance:
            result = successful_llguidance[0]
            print(f"llguidance:   '{result.output}' (JSON: {'‚úÖ' if result.valid_json else '‚ùå'})")
            print(f"              Log-probability: N/A")
        
        # Final verdict
        print(f"\nüèÜ FINAL VERDICT")
        print("-" * 17)
        
        if stolcke_success_rate > llguidance_success_rate:
            print("üéØ Stolcke PCFG wins on reliability")
        elif llguidance_success_rate > stolcke_success_rate:
            print("üéØ llguidance wins on reliability") 
        else:
            print("ü§ù Both equally reliable")
        
        if stolcke_avg_time > 0 and llguidance_avg_time > 0:
            if llguidance_avg_time < stolcke_avg_time:
                ratio = stolcke_avg_time / llguidance_avg_time
                print(f"‚ö° llguidance wins on speed ({ratio:.1f}x faster)")
            elif stolcke_avg_time < llguidance_avg_time:
                ratio = llguidance_avg_time / stolcke_avg_time
                print(f"‚ö° Stolcke wins on speed ({ratio:.1f}x faster)")
            else:
                print("‚ö° Speed is comparable")
        
        print("üìä Stolcke's unique advantage: Grammar log-probabilities")
        print("üéØ Both solve tokenization brittleness completely")


def main():
    """Run proper llguidance comparison."""
    
    comparison = ProperLLGuidanceComparison()
    results = comparison.run_comparison(trials=2)
    
    print(f"\nüéØ This benchmark compares production-ready approaches to")
    print(f"grammar-constrained generation. Both solve tokenization issues.")


if __name__ == "__main__":
    main()